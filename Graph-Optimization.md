[editor on GitHub](https://github.com/fsword73/jianyang.github.io/edit/master/Graph-Optimization.md)

# [Inside Intelâ€™s nGraph, a Universal Deep Learning Compiler](https://www.datanami.com/2018/04/26/inside-intels-ngraph-a-universal-deep-learning-compiler/)
# [Writting an LLVM Backend](https://www.llvm.org/docs/WritingAnLLVMBackend.html)
# [Tutorial: Creating an LLVM Backend for the Cpu0 Architecture](http://jonathan2251.github.io/lbd/)
# [Glow: Graph Lowering Compiler Techniques for Neural Networks](https://github.com/pytorch/glow)
# [DLVM: Modern Compiler Infrastructure for Deep Learning Systems](http://dlvm.org/)
# [Tensor Comprehensions](https://github.com/facebookresearch/TensorComprehensions)
# [The Tensor Algebra Compiler (taco) computes tensor expressions on sparse and dense tensors](http://tensor-compiler.org/)
# [Tiramisu: A Code Optimization Framework](https://github.com/rbaghdadi/tiramisu/)
### [Tiramisu: A Code Optimization Framework for High Performance Systems](https://arxiv.org/pdf/1804.10694.pdf)
# [PlaidML:A platform for making deep learning work everywhere](https://github.com/plaidml/plaidml)
####  [Tensor Compilers: Comparing PlaidML, Tensor Comprehensions, and TVM](http://vertex.ai/blog/compiler-comparison)
# [Diesel: DSL for linear algebra and neural net computations on GPUs]
# [FGen: High-Performance Convolution Generator](https://astojanov.github.io/projects/fgen/)
# [The streaming rollout of deep networks - towards fully model-parallel execution](https://arxiv.org/abs/1806.04965) [sourcode](https://github.com/boschresearch/statestream)
